---
layout: post
title: 002-Budget Planning for Breeder Agents
subtitle: Give agents money
#gh-repo: daattali/beautiful-jekyll
comments: true
author: Colton G
---


In the first experiment, our agent was given a breeding program with a fixed budget; The same number of offspring were produced each generation. In reality, the breeder can allocate resources unevenly across the selection cycles. The simplest way to allocate budget is to increase the number of grown individuals, giving them more shuffled genomes and potentially higher trait scores.

This presents an interesting extension for our breeder agent. We will now give it a starting budget and each cycle it can decide how much of the current budget to spend on the given cycle.

The goal is to see where the breeder agent will exploit larger populations with this budget. At minimum, we will give each cycle 10 free offspring, however with a budget of 5000... any given cycle could have between 10 and 5010 offspring in theory.

First we will calculate the baseline; the trait score that will be a good approximate for our agent to reproduce (or beat).

In this case, we have 2 variables, so will produce a heatmap where the x axis is the selection intensity (same as in first blog post) and the y axis is the budget allocation (% of remaining budget to spend on the current cycle).

We can see a clear landscape of optimal values peaking at low action values for both selection intensity and budget allocation per cycle.

Similar to the last experiment, it is likely that this simple baseline is actually a strong solution. Any solution which effectively uses the total budget and burns off most of the genetic variance will likely be similar in final score...

![image](https://github.com/cjGO/cjgo.github.io/blob/master/assets/img/blog_budget_heatmap.png?raw=true)


Training an agent with two actions instead of one is actually much more complicated. By doubling the number of decisions the agent must make each cycle, the training logs are a lot more erratic, experiencing a form of RL training collapse where we can see the values dropping off, and losing variation.

Surprisingly, after collapsing, the agent is apparently able to recover given enough time, however this is by and large an unstable process.

![image](https://github.com/cjGO/cjgo.github.io/blob/master/assets/img/budgetbot_02_logs.png?raw=true)

Note: the selection_intensity per generation and budget_action_per_generation shows the agents breeding decisions per cycle.

Focusing on the training log where the max phenotype (yellow line) approaches the red dashed line (the baseline target), we can see that the agent almost successfully reaches the baseline score. The initial budget spending per cycle (bottom chart) quickly redistributes. The overall pattern though is clear; spending money at the beginning of the breeding program is important. The key takeaway for a breeder is that the early cycles should have larger investments relative to the start. There is a linear mapping of spending less for every subsequent cycle to exploit the budget. This experiment emphasizes that breeders need to start strong; you may not be able to save a breeding program if you did not invest properly early on!

---

Some concerns with this workflow which may be contributing to this instability...

We are using the stable-baselines 3 library's built-in policy network which influences the action output, especially at the very beginning of training. This is most obvious in the last chart for budget_spent_per_generation. The default settings favor the very first values from the network to be centered around 0 which translates to spending 50% of the remaining budget on the first cycle. Over time this balances out and it appears the agent finds a more logical budget distribution, however this initial settings may have strong influences on the rest of training preventing it from finding a more consistent tracjetory through the action space. To sum it up, the training is possibly initalized in an unfavorable starting position of the action space which could have a negative domino effect for the rest of the training process.

We can code our own policies and control the initial weights and biases so initial values start out much lower, potentially providing flexibility to the rest of the training process.

We may also hand-code some behavior like constraints or penalties to avoid behavior explicitly. For example, we can punish the agent if the actions within an episode are too similar (like what clearly happens after a collapse event).

It's unfortunate the agent does not learn a clearly stronger strategy that beats the baseline which uses constant actions for both selection_intensity and budget_allocation. However, like the first experiment, it may be too simple and simply spending all the budget and burning all the genetic variance is going to give excellent solutions.

There are also concepts like curriculum learning which are popular. This is based on gradually introducing complexity to the agent's environment throughout the training process.

More time will be spent with this environment, to see if any tricks positively affects the agent's training behavior and overall performance.
